第6章 时序差分学习
==================

如果必须将一个想法确定为强化学习的核心和新颖，那么毫无疑问它将是 *时序差分* （TD）学习。
TD学习是蒙特卡洛思想和动态规划（DP）思想的结合。与蒙特卡洛方法一样，TD方法可以直接从原始经验中学习，而无需环境动态模型。
与DP一样，TD方法部分基于其他学习估计更新估计，而无需等待最终结果（它们是自举）。
TD，DP和蒙特卡洛方法之间的关系是强化学习理论中反复出现的主题；本章是我们开始探索这个关系。
在我们完成之前，我们将看到这些想法和方法相互融合，并且可以以多种方式组合。
特别是，在第7章中，我们介绍了n步算法，它提供了从TD到蒙特卡洛方法的桥梁，
在第12章中我们介绍了 TD(:math:`\lambda`) 算法，它无缝地统一了它们。

像往常一样，我们首先关注策略评估或 *预测* 问题，即估算给定策略 :math:`\pi` 的价值函数 :math:`v_\pi` 的问题。
对于控制问题（找到最优策略），DP、TD和蒙特卡洛方法都使用广义策略迭代（GPI）的一些变体。
方法的差异主要在于它们对预测问题的方法的差异。


6.1 TD预测
-------------

TD和蒙特卡罗方法都使用经验来解决预测问题。对于基于策略 :math:`\pi` 的一些经验，
两种方法都更新了他们对该经验中发生的非终结状态 :math:`S_t` 的 :math:`v_\pi` 的估计 :math:`V`。
粗略地说，蒙特卡罗方法一直等到访问后的回报已知，然后使用该回报作为 :math:`V(S_t)` 的目标。
适用于非平稳环境的简单的每次访问蒙特卡罗方法是

.. math::

    V(S_{t}) \leftarrow V(S_{t})+\alpha\left[ G_{t}-V(S_{t})\right]
    \tag{6.1}

其中 :math:`G_t` 是跟随时间t的实际回报，:math:`\alpha` 是一个恒定的步长参数（参见方程2.4）。
我们将此方法称为恒定 :math:`\alpha` MC。
蒙特卡罗方法必须等到回合结束才能确定 :math:`V(S_t)` 的增量（这时只有 :math:`G_t` 已知），
TD方法需要等到下一个时间步。 在时间 :math:`t+1`，它们立即形成目标并
使用观察到的奖励 :math:`R_{t+1}`和估计 :math:`V(S_{t+1})` 进行有用的更新。
最简单的TD方法立即进行更新

.. math::

    V(S_{t}) \leftarrow V(S_{t})+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_{t})\right]
    \tag{6.2}

过渡到 :math:`S_{t+1}` 并接收 :math:`R_{t+1}`。
在实际中，蒙特卡洛更新的目标是 :math:`G_t`，而TD更新的目标是 :math:`R_{t+1} + \gamma V(S_{t+1})`。
这种TD方法称为 *TD(0)* 或 *一步TD*，因为它是第12章和第7章中开发的 TD(:math:`\lambda`)和n步TD方法的特例。
下面的方框完全以程序形式给出了TD(0)。

.. admonition:: 表格TD(0)估计 :math:`v_\pi`
    :class: important

    输入：要评估策略 :math:`\pi`

    算法参数：步长 :math:`\alpha in (0,1]`

    对所有 :math:`s \in \mathbb{S}^{+}`，除了 :math:`V(终点)=0`，任意初始化 :math:`V(s)`

    对每个回合循环：

        初始化 :math:`S`

        对回合的每一步循环：

            :math:`A \leftarrow` 由 :math:`\pi` 给出 :math:`S` 的动作

            采取动作 :math:`A`，观察 :math:`R`，:math:`S^{\prime}`

            :math:`V(S) \leftarrow V(S)+\alpha\left[R+\gamma V(S^{\prime})-V(S)\right]`

            :math:`S \leftarrow S^{\prime}`

        直到 :math:`S` 是终点

因为TD(0)部分基于现有估计进行更新，所以我们说它是一种 *自举（bootstrapping）* 方法，就像DP一样。
我们从第3章知道

.. math::

    \begin{align}
    v_{\pi}(s) & \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right]  \tag{6.3}\\
    &=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} | S_{t}=s\right] & (由(3.9))\\
    &=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right] \tag{6.4}
    \end{align}

粗略地说，蒙特卡罗方法使用（6.3）的估计作为目标，而DP方法使用（6.4）的估计作为目标。
蒙特卡洛目标是估计，因为（6.3）中的预期值未知；使用样本回报来代替实际预期回报。
DP目标是一个估计，不是因为完全由环境模型提供的预期值，而是因为 :math:`v_{\pi}(S_{t+1})` 未知，
且使用当前估计值 :math:`V(S_{t+1})` 来代替。
TD目标是估计原因有两个：它在（6.4）中对预期值进行采样，*并* 使用当前估计值 :math:`V` 而不是真实 :math:`v_\pi`。
因此，TD方法将蒙特卡罗的采样与DP的自举相结合。
正如我们将要看到的那样，通过谨慎和想象，这将使我们在获得蒙特卡罗和DP方法的优势方面走得很远。

.. figure:: images/TD(0).png
    :align: right
    :width: 50px

右侧是表格TD(0)的备份图。
备份图顶部的状态节点的值估计基于从它到紧接的状态的一个样本转换而更新。
我们将TD和蒙特卡洛更新称为样本更新，因为它们涉及展望样本后继状态（或状态-动作对），
使用后继值和相应的奖励来计算备份值（?），然后相应地更新原始状态（或状态-动作对）的值。
*样本* 更新与DP方法的 *预期* 更新不同，因为它们基于单个样本后继，而不是基于所有可能后继的完整分布。

最后，请注意在TD(0)更新中，括号中的数量是一种误差，
衡量 :math:`S_t` 的估计值与更好的估计 :math:`R_{t+1} + \gamma V(S_{t+1})` 之间的差异。
这个数量称为 *TD误差*，在整个强化学习过程中以各种形式出现：

.. math::

    \delta_{t} \doteq R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)
    \tag{6.5}

请注意，每次TD误差都是 *当时估算* 的误差。因为TD误差取决于下一个状态和下一个奖励，所以直到一个步骤之后才可用。
也就是说，:math:`\delta_{t}` 是 :math:`V(S_{t+1})` 中的误差，在时间 :math:`t+1` 可用。
还要注意，如果列表 :math:`V` 在回合期间没有改变（因为它不是蒙特卡罗方法(?)），那么蒙特卡罗误差可以写成TD误差的和：

.. math::

    \begin{align}
    G_{t}-V\left(S_{t}\right) &=R_{t+1}+\gamma G_{t+1}-V\left(S_{t}\right)+\gamma V\left(S_{t+1}\right)-\gamma V\left(S_{t+1}\right) & (由(3.9)) \\
    &=\delta_{t}+\gamma\left(G_{t+1}-V\left(S_{t+1}\right)\right) \\
    &=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2}\left(G_{t+2}-V\left(S_{t+2}\right)\right) \\
    &=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2} \delta_{t+2}+\cdots+\gamma^{T-t-1} \delta_{T-1}+\gamma^{T-t}\left(G_{T}-V\left(S_{T}\right)\right) \\
    &=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2} \delta_{t+2}+\cdots+\gamma^{T-t-1} \delta_{T-1}+\gamma^{T-t}(0-0) \\
    &=\sum_{k=t}^{T-1} \gamma^{k-t} \delta_{k} \tag{6.6}
    \end{align}

如果在回合期间更新 :math:`V` （因为它在TD(0)中），则此恒等式不准确，但如果步长很小，那么它可能仍然保持近似。
这种恒等式的一般化在时序差分学习的理论和算法中起着重要作用。

*练习6.1* 如果 :math:`V` 在回合中发生变化，那么（6.6）只能保持近似；等式两边的区别是什么？
设 :math:`V_t` 表示在TD误差（6.5）和TD更新（6.2）中在时间 :math:`t` 使用的状态值列表。
重做上面的推导以确定必须添加到TD误差总和的额外量，以便等于蒙特卡罗误差。

**例 6.1 开车回家** 每天下班回家后，你都会尝试预测回家需要多长时间。
当你离开办公室时，你会记下时间，星期几，天气以及其他可能相关的内容。
这个星期五你正好在6点钟离开，你估计要回家需要30分钟。当你到达你的车是6:05，你注意到开始下雨了。
在雨中交通通常较慢，所以你需要花费35分钟，或者总共40分钟。十五分钟后，你及时完成了旅程的高速公路部分。
当你驶出高速进入第二部分道路时，你将总旅行时间的估计值减少到35分钟。
不幸的是，此时你被困在一辆慢卡车后面，而且道路太窄而无法通过。
你最终不得不跟随卡车，直到6:40你转到住的小街。三分钟后你就回家了。因此，状态，时间和预测的顺序如下：

======================= ==================== ================= ===============
状态                      经过时间（分钟）        预测到的时间       预计总时间
======================= ==================== ================= ===============
周五6点离开办公室            0                      30              30
到达车，下雨                 5                      35              40
驶出高速公路                 20                     15              35
第二条路，在卡车后面          30                     10              40
进入家的街道                 40                     3              43
到家                        43                     0              43
======================= ==================== ================= ===============

这个例子中的奖励是旅程每一段的经过时间 [1]_。我们不打折（:math:`\gamma=1`），因此每个状态的回报是从该状态开始的实际时间。
每个状态的价值是 *预期的* 时间。第二列数字给出了遇到的每个状态的当前估计值。

查看蒙特卡罗方法操作的一种简单方法是绘制序列上预测的总时间（最后一列），如图6.1（左）所示。
红色箭头表示常量 :math:`\alpha` MC方法（6.1）推荐的预测变化，其中 :math:`\alpha=1`。
这些正是每个状态的估计值（预测的行走时间）与实际返回（实际时间）之间的误差。
例如，当你离开高速公路时，你认为回家仅需15分钟，但实际上需要23分钟。
公式6.1适用于此点，并确定驶出公路后的估计时间的增量。
误差 :math:`G_t - V(S_t)` 此时为8分钟。假设步长参数 :math:`\alpha` 为 :math:`1/2`。
然后，由于这种经验，退出高速公路后的预计时间将向上修改四分钟。
在这种情况下，这可能是一个太大的变化；卡车可能只是一个不幸的中断。
无论如何，只有在你到家之后才能进行变更。只有在这一点上你才知道任何实际的回报。

.. figure:: images/figure-6.1.png

    **图6.1** 通过蒙特卡罗方法（左）和TD方法（右）在开车回家示例中推荐的变化。

在学习开始之前，是否有必要等到最终结果已知？
假设在另一天你再次估计离开你的办公室时需要30分钟才能开车回家，但是你会陷入大规模的交通堵塞之中。
离开办公室后二十五分钟，你仍然在高速公路上等待。你现在估计还需要25分钟才能回家，共计50分钟。
当你在车流中等待时，你已经知道你最初估计的30分钟过于乐观了。
你必须等到回家才增加对初始状态的估计吗？根据蒙特卡罗的方法，你必须，因为你还不知道真正的回报。

另一方面，根据TD方法，你可以立即学习，将初始估计值从30分钟转移到50分。
事实上，每个估计值都会转移到紧随其后的估计值。
回到驾驶的第一天，图6.1（右）显示了TD规则（6.2）推荐的预测变化
（如果 :math:`\alpha=1`，这些是规则所做的更改）。
每个误差与预测随时间的变化成比例，即与预测的 *时序差分* 成比例。

除了在车流中等待你做点什么之外，还有几个计算原因可以解释为什么根据你当前的预测学习是有利的，
而不是等到你知道实际回报时才终止。我们将在下一节简要讨论其中的一些内容。

*练习6.2* 这是一个练习，以帮助你发展直觉，了解为什么TD方法通常比蒙特卡罗方法更有效。
考虑开车回家示例以及如何通过TD和蒙特卡罗方法解决它。你能想象一个TD更新平均比蒙特卡罗更新更好的情景吗？
举一个示例场景 - 过去经验和当前状态的描述 - 你期望TD更新更好。这里有一个提示：假设你有很多下班开车回家的经验。
然后你搬到一个新的建筑物和一个新的停车场（但你仍然在同一个地方进入高速公路）。现在你开始学习新建筑的预测。
在这种情况下，你能看出为什么TD更新可能会好得多，至少初始是这样吗？在原始场景中发生同样的事情可能吗？


6.2 TD预测方法的优势
---------------------

TD方法部分基于其他估计更新其估计。他们通过猜测来学习猜测 - 他们 *引导*。这是一件好事吗？
TD方法与蒙特卡罗和DP方法相比有哪些优势？开发和回答这些问题将涉及本书的其余部分以及更多内容。
在本节中，我们简要地预测一些答案。

显然，TD方法比DP方法具有优势，因为它们不需要环境模型，其奖励和下一状态概率分布。

TD方法相对于蒙特卡罗方法的下一个最明显的优势是它们自然地以在线，完全递增的方式实现。
使用蒙特卡罗方法，必须等到回合的结束，因为只有这样才能知道回报，而使用TD方法，只需要等待一个时间步。
令人惊讶的是，这通常是一个重要的考虑因素。一些应用程序有很长的回合，所以延迟所有学习直到回合结束太慢。
其他应用程序是持续的任务，根本没有回合。最后，正如我们在前一章中所提到的，
一些蒙特卡罗方法必须忽略或折扣采取实验行动的事件，这可能会大大减慢学习速度。
TD方法不太容易受到这些问题的影响，因为无论采取何种后续行动，它们都会从每次转变中学习。

但TD方法听起来有效吗？当然，从下一个中学习一个猜测是方便的，而不是等待实际的结果，
但我们仍然可以保证收敛到正确的答案吗？令人高兴的是，答案是肯定的。
对于任何固定策略 :math:`\pi`，已经证明TD(0)收敛到 :math:`v_{\pi}`，
如果它足够小，则表示恒定步长参数，如果步长参数按照通常随机近似条件（2.7）减小，则概率为1（译者注：这句没太明白）。
大多数收敛证明仅适用于上面（6.2）所述算法的基于表格的情况，但是一些也适用于一般线性函数逼近的情况。
这些结果将在9.4节的更一般性设置中讨论。

如果TD和蒙特卡罗方法渐近地收敛到正确的预测，那么自然下一个问题是“哪个首先收敛到那里？”
换句话说，哪种方法学得更快？哪种方法使得有限数据的使用更加有效？目前，这是一个悬而未决的问题，
即没有人能够在数学上证明一种方法比另一种方法收敛得更快。事实上，甚至不清楚说出这个问题的最恰当的正式方式是什么！
然而，在实践中，通常发现TD方法比常数- :math:`\alpha` MC方法在随机任务上收敛得更快，如例6.2所示。

.. admonition:: 例6.2 随机行走
    :class: important

    在这个例子中，我们在应用于以下马尔可夫奖励过程时，凭经验比较TD(0)和常数- :math:`alpha` MC的预测能力：

    .. figure:: images/random_walk_markov_reward_process.png

    *马尔可夫奖励过程* （MRP）是没有行动的马尔可夫决策过程。我们经常在关注预测问题时使用MRP，
    其中不需要将由环境引起的动态与由个体引起的动态区分开来。
    在该MRP中，所有回合以中心状态 :math:`C` 开始，然后以相同的概率在每一步上向左或向右前进一个状态。
    回合终止于最左侧或最右侧。当回合在右边终止时，会产生 :math:`+1` 的奖励；所有其他奖励都是零。
    例如，典型的回合可能包含以下状态和奖励序列：:math:`C, 0, B, 0, C, 0, D, 0, E, 1`。
    因为此任务是未折扣的，所以每个状态的真实价值是从该状态开始在右侧终止的概率。
    因此，中心状态的真值是 :math:`v_\pi(C)=0.5`。所有状态 :math:`A` 到 :math:`E` 的
    真实价值都是 :math:`\frac{1}{6}`，:math:`\frac{2}{6}`，:math:`\frac{3}{6}`，
    :math:`\frac{4}{6}` 和 :math:`\frac{5}{6}`。

    .. figure:: images/random_walk_comparison.png

    上面的左图显示了在TD(0)的单次运行中在不同数量的回合之后学习的价值。
    100回合之后的估计值与它们的真实值接近 - 具有恒定的步长参数（在此示例中 :math:`\alpha=0.1`），
    这些值随着最近一个回合的结果而无限地波动。右图显示了两种方法对于各种 :math:`\alpha` 值的学习曲线。
    显示的性能度量是学习的值函数和真值函数之间的均方根（RMS）误差，在五个状态上取平均值，
    然后在超过100次运行上平均。在所有情况下，对于所有 :math:`s`，近似值函数被初始化为中间值 :math:`V(s)=0.5`。
    在这项任务中，TD方法始终优于MC方法。

*练习6.3* 从随机游走示例的左图中显示的结果看来，第一回合仅导致 :math:`V(A)` 的变化。
这告诉你第一回合发生了什么？为什么只有这一状态的估计值发生了变化？确切地说它改变了多少？

*练习6.4* 随机游走示例右图中显示的特定结果取决于步长参数 :math:`\alpha` 的值。
如果使用更广范围的 :math:`\alpha` 值，您认为关于哪种算法更好的结论是否会受到影响？
是否存在不同的固定值 :math:`\alpha`，其中任何一种算法的表现都要比显示的好得多？为什么或者为什么不？

*\*练习6.5* 在随机游走示例的右图中，TD方法的RMS误差似乎下降然后再上升，特别是在 :math:`\alpha` 高时。
可能是什么导致了这个？你认为这总是会发生，或者它可能是近似值函数初始化的函数吗？

*练习6.6* 在例6.2中，我们说状态 :math:`A` 到 :math:`E` 随机游走示例的
真实值是 :math:`\frac{1}{6}`，:math:`\frac{2}{6}`，:math:`\frac{3}{6}`，
:math:`\frac{4}{6}` 和 :math:`\frac{5}{6}`。描述至少两种不同的方式相说明这些可以计算出来。
您认为我们实际使用哪个？为什么？


6.3 TD(0)的最优性
------------------

假设只有有限的经验，比如10个回合或100个时间步。在这种情况下，使用增量学习方法的常见方法是重复呈现经验，直到该方法收敛于答案。
给定近似值函数 :math:`V`，对于访问非终结状态的每个时间步长 :math:`t` 计算由（6.1）或（6.2）指定的增量，
但是值函数仅通过所有增量的总和改变一次。
然后，使用新的值函数再次处理所有可用的经验，以产生新的整体增量，依此类推，直到值函数收敛。
我们将此称为 *批量更新*，因为只有在处理完每 *批* 完整的训练数据后才会进行更新。

在批量更新中，TD(0)确定性地收敛到与步长参数 :math:`\alpha` 无关的单个答案，只要选择 :math:`\alpha` 足够小。
常数- :math:`\alpha` MC方法也在相同条件下确定性地收敛，但是收敛到不同的答案。
理解这两个答案将有助于我们理解两种方法之间的差异。在正常更新下，方法不会一直移动到各自的批次答案，
但在某种意义上，他们会在这些方向上采取措施。在尝试理解一般的两个答案之前，对于所有可能的任务，我们首先看一些例子。

**例6.3 批量更新下的随机行走** 如下将TD(0)和常数- :math:`\alpha` MC的批量更新版本应用于随机行走预测示例（示例6.2）。
在每一新回合之后，到目前为止所见的所有回合都被视为一个批量。它们被重复地呈现给算法 TD(0)或常数- :math:`\alpha` MC，
其中 :math:`\alpha` 足够小，使得价值函数收敛。然后将得到的价值函数与 :math:`v_\pi` 进行比较，
绘制五个状态（以及整个实验的100次独立重复）的平均均方根误差，得到图6.2所示的学习曲线。
请注意，批处理TD方法始终优于批量蒙特卡罗方法。

.. figure:: images/figure-6.2.png
    :align: right
    :width: 400px

    **图6.2** 在随机行走任务的批量训练下TD(0)和常数- :math:`\alpha` MC的性能。

在批量训练中，常数- :math:`\alpha` MC收敛于值 :math:`V(s)`，这是在访问每个状态之后经历的实际回报的样本平均值。
这些是最佳估计，因为它们最小化了训练集中实际回报的均方误差。从这个意义上来说，令人惊讶的是，
批量TD方法能够根据右图所示的均方根误差测量得到更好的效果。批量TD如何能够比这种最佳方法表现更好？
答案是蒙特卡罗方法仅以有限的方式是最优的，而TD以与预测回报更相关的方式是最优的。

**例6.4 你是预测者** 现在把自己置于未知马尔可夫奖励过程的回报预测者的角色。假设你观察了以下八个回合：

======== =====
A,0,B,0   B,1
B,1       B,1
B,1       B,1
B,1       B,0
======== =====

这意味着第一个回合在状态A开始，转换为B，奖励为0，然后从B终止，奖励为0。其他七个回合甚至更短，从B开始并立即终止。
鉴于这批数据，您认为估计 :math:`V(A)` 和 :math:`V(B)` 的最佳预测最佳值是什么？
每个人都可能会同意 :math:`V(B)` 的最佳值是 :math:`\frac{3}{4}`，
因为在状态B的8次中有6次过程立即终止，回报为1，而在B中另外两次过程终止于回报0。

.. figure:: images/you_are_the_predictor.png
    :align: right
    :width: 150px

但是，根据这些数据，估算 :math:`V(A)` 的最佳值是多少？这里有两个合理的答案。
一个是观察到过程处于状态A会100％立即到达B（奖励为0）；
因为我们已经确定B的值为 :math:`\frac{3}{4}`，所以A的值也必须为 :math:`\frac{3}{4}`。
查看这个答案的一种方法是它首先建立马尔可夫过程的建模，在这种情况下如右图所示，
然后计算给定模型的正确估计，在这种情况下确实给出 :math:`V(A)=\frac{3}{4}`。
这也是批量TD(0)给出的答案。

另一个合理的答案就是注意到我们已经看过A一次，其后的回报是0；因此，我们估计 :math:`V(A)` 为0。
这是批量蒙特卡罗方法给出的答案。请注意，它也是给出训练数据最小平方误差的答案。
实际上，它给数据带来零误差。但我们仍然希望第一个答案会更好。如果该过程是马尔可夫的，
我们预计第一个答案将对 *未来* 数据产生较低的误差，即使蒙特卡罗对现有数据的回答更好。

实施例6.4说明了批次TD(0)和批量蒙特卡罗方法发现的估计值之间的一般差异。
批量蒙特卡罗方法总是找到最小化训练集上的均方误差的估计，
而批量TD(0)总是找到对马尔可夫过程的最大似然模型完全正确的估计。
通常，参数的 *最大似然估计* 是其生成数据的概率最大的参数值。
在这种情况下，最大似然估计是从观察到的事件中以明显方式形成的马尔可夫过程的模型：
从 :math:`i` 到 :math:`j` 的估计转移概率是从 :math:`i` 到 :math:`j` 的观察到的转变的分数，
以及相关联的预期奖励是在这些转变中观察到的奖励的平均值。
给定此模型，如果模型完全正确，我们可以计算值函数的估计值，该估计值将完全正确。
这被称为 *确定性等价估计*，因为它等同于假设潜在过程的估计是确定的而不是近似的。
通常，批量TD(0)收敛于确定性等价估计。

这有助于解释为什么TD方法比蒙特卡罗方法更快收敛。在批量形式中，TD(0)比蒙特卡罗方法更快，
因为它计算真实的确定性等价估计。这解释了随机行走任务中批量结果显示的TD(0)的优势（图6.2）。
与确定性等价估计的关系也可以部分解释非批量TD(0)的速度优势（例如，例6.2，第125页，右图）。
尽管非批量方法没有达到确定性等效或最小平方误差估计，但它们可以被理解为大致在这些方向上移动。
非批量TD(0)可能比常数- :math:`\alpha` MC更快，因为它正朝着更好的估计方向发展，即使它没有完全到达那里。
目前，关于在线TD和蒙特卡罗方法的相对效率，没有更明确的说法。

最后，值得注意的是，尽管确定性等价估计在某种意义上是最优解，但直接计算它几乎是不可行的。
如果 :math:`n=|\mathcal{S}|` 是状态的数量，然后仅形成过程的最大似然估计可能需要 :math:`n^2` 个存储器的量级，
并且如果按常规方式完成，则计算相应的值函数需要 :math:`n^3` 个计算步骤的量级。
在这些术语中，确实令人惊讶的是TD方法可以使用不超过 :math:`n` 阶的存储器和训练集上的重复计算来近似相同的解。
在具有大状态空间的任务中，TD方法可能是近似确定性等价解的唯一可行方法。

*\*练习6.7* 设计TD（0）更新的非策略版本，可以与任意目标策略⇡一起使用并覆盖行为策略b，在每个步骤t使用重要性采样率（5.3）。


6.4 Sarsa：在策略TD控制
------------------------

我们现在转向使用TD预测方法来解决控制问题。像往常一样，我们遵循广义策略迭代（GPI）的模式，这次只使用TD方法解决评估或预测部分。
与蒙特卡罗方法一样，我们面临着对探索和利用进行权衡的需要，并且再次采用的方法分为两大类：在策略和离策略。
在本节中，我们将介绍在策略上的TD控制方法。

第一步是学习动作价值函数而不是状态价值函数。特别是，对于在策略的方法，
我们必须估计当前行为策略 :math:`\pi` 和所有状态 :math:`s` 和行动 :math:`a` 的 :math:`q(s, a)`。
这可以使用基本上与上述用于学习 :math:`v_\pi` 的相同的TD方法来完成。
回想一下，回合由一系列状态和状态-动作对组成：

.. figure:: images/sequence_of_states_and_state-action_pairs.png

在上一节中，我们考虑了从状态到状态的转变，并学习了状态的价值。
现在我们考虑从状态-动作对转换到状态-动作对，并学习状态-动作对的价值。
这些案例在形式上是相同的：它们都是具有奖励过程的马尔可夫链。
确保TD(0)下状态价值收敛的定理也适用于相应的动作价值算法：

.. math::

    Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right].
    \tag{6.7}

.. figure:: images/backup_of_sarsa.png
    :width: 50px
    :align: right

在从非终点状态 :math:`S_t` 的每次转换之后完成该更新。
如果 :math:`S_{t+1}` 是终点，则 :math:`Q(S_{t+1}, A_{t+1})` 被定义为零。
此规则使用五元组事件的每个元素 :math:`\left(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1}\right)`，
它们构成从一个状态-动作对到下一个状态-动作对的转换。这个五元组产生了算法的名称Sarsa。
Sarsa的备份图如右图所示。

可以直接设计基于Sarsa预测方法的在策略控制算法。
正如在所有策略方法中一样，我们不断估计行为策略 :math:`\pi` 的 :math:`q_\pi`，
同时将 :math:`\pi` 改为 :math:`q_\pi` 的贪婪。Sarsa控制算法的一般形式在下面的框中给出。

Sarsa算法的收敛属性取决于策略对 :math:`Q` 的依赖性。
例如，可以使用 :math:`\varepsilon` -贪婪或 :math:`\varepsilon` -soft 策略。
只要所有状态-动作对被无限次访问并且策略收敛于贪婪策略的限制
（可以控制，例如，设置 :math:`\varepsilon` -贪婪时的 :math:`\varepsilon=1/t`），
Sarsa就以概率1收敛到最优策略和动作-价值函数。

*练习6.8* 显示（6.6）的动作价值适用于TD误差的动作值形式
:math:`\delta_{t}=R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)`，
再次假设值不会逐步变化。

.. admonition:: Sarsa （在策略TD控制）估计 :math:`Q \approx q_*`
    :class: important

    算法参数：步长 :math:`\alpha \in (0,1]`，小值 :math:`\varepsilon > 0`

    对所有 :math:`s \in \mathcal(S)^+`，:math:`a \in \mathcal(A)(s)`，任意初始 :math:`Q(s, a)`，除了 :math:`Q(终点, \cdot)=0`

    对每一个回合循环：

        初始化 :math:`S`

        使用从 :math:`Q` 派生的策略从 :math:`S` 中选择 :math:`A`（例如，:maht:`\varepsilon` -贪婪）

        对回合的每一步循环：

            采取动作 :math:`A`，观察 :math:`R`, :math:`S^{\prime}`

            使用从 :math:`Q` 派生的策略从 :math:`S^{\prime}` 中选择 :math:`A^{\prime}`（例如，:maht:`\varepsilon` -贪婪）

            :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right]`

            :math:`S \leftarrow S^{\prime}`；:math:`A \leftarrow A^{\prime}`；

        直到 :math:`S` 是终点

**例6.5：有风网格世界** 下面的插图是一个标准的网格世界，有开始和目标状态，
但有一个差异：在网格中间有一个向上运行的侧风。
动作是标准的四个── **上**，**下**，**右** 和 **左**，但在中间区域，
结果的下一个状态向上移动一个“风”，其强度因列而异。
在每列下方给出风的强度，向上移动的格子数量。
例如，如果你是目标右侧的一个单元格，则左侧的操作会将你带到目标上方的单元格。
这是一个没有折扣的回合任务，在达到目标状态之前回报恒定为 :math:`-1`。

.. figure:: images/sarsa_for_windy_gridworld.png
    :width: 350px
    :align: right

右边的图表显示了将 :math:`\varepsilon` -贪婪Sarsa应用于此任务的结果，
其中 :math:`\varepsilon=0.1`，:math:`\alpha=0.5`，
并且所有 :math:`s, a` 初始化价值 :math:`Q(s,a)=0`。
图表的斜率增加表明目标随着时间的推移更快地达到。经过8000个时间步骤，贪婪的策略早已是最优的（它的轨迹显示在图中）；
继续的 :math:`\varepsilon` -贪婪探索将平均回合长度保持在17步左右，比最低值15更多两步。
请注意，蒙特卡罗方法在这里不能轻易使用，因为所有策略都不能保证终止。
如果发现某项策略导致个体保持相同的状态，然后下一回合就永远不会结束。
Sarsa等在线学习方法没有这个问题，因为他们很快就会在这一回合中说这些策略很差，并转而使用其他策略。

*练习6.9：带有对角移动的有风网格世界（编程）* 重新解决有风的网格世界，假设有八种可能的动作，
包括对角线移动，而不是通常的四种动作。额外行动能使你做得好多少？除了由风引起的第九次动作之外，你能做到更好吗？

*练习6.10：随机风（编程）* 用对角移动重新解决有风网格世界任务，假设风的效果（如果有的话）是随机的，
有时从每列给出的平均值变化1。也就是说，三分之一的时间里你完全按照这些值移动，如上一个练习中所示，
但也有三分之一的时间将在该单元格一个单元格之上移动，另外三分之一的时间在一个单元格之下移动。
例如，如果你是目标右侧的一个单元格并向 **左** 移动，那么三分之一的时间将移动到目标上方一个单元格，
三分之一的时间将移动到目标上方两个单元格，最后三分之一的时间你移动到目标。


6.5 Q-learning：离策略TD控制
-----------------------------

强化学习的早期突破之一是开发了一种名为 *Q-learning* （Watkins，1989）的离策略TD控制算法，由以下定义：

.. math::

    Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right].
    \tag{6.8}

在这种情况下，学习的动作-价值函数 :math:`Q` 直接近似 :math:`q_*`，即最佳动作-价值函数，与所遵循的策略无关。
这极大地简化了算法的分析并实现了早期的收敛证明。该策略仍然具有一个效果，即它确定访问和更新哪些状态-动作对。
但是，正确收敛所需的只是所有动作-价值对继续更新。正如我们在第5章中所观察到的那样，这是一个最小要求，
因为在一般情况下保证找到最佳行为的任何方法都必须要求它。
在该假设和步长参数序列的通常随机近似条件的变体下，:math:`Q` 已经显示出以概率1收敛到 :math:`q_*`。
Q-learning算法以程序形式显示如下。

.. admonition:: Q-learning （离策略TD控制）估计 :math:`\pi \approx \pi_*`
    :class: important

    算法参数：步长 :math:`\alpha \in (0,1]`，小值 :math:`\varepsilon > 0`

    对所有 :math:`s \in \mathcal(S)^+`，:math:`a \in \mathcal(A)(s)`，任意初始 :math:`Q(s, a)`，除了 :math:`Q(终点, \cdot)=0`

    对每一个回合循环：

        初始化 :math:`S`

        对回合的每一步循环：

            使用从 :math:`Q` 派生的策略从 :math:`S` 中选择 :math:`A`（例如，:maht:`\varepsilon` -贪婪）

            采取动作 :math:`A`，观察 :math:`R`, :math:`S^{\prime}`

            :math:`Q(S, A) \leftarrow Q(S, A)+\alpha\left[R+\gamma \max _{a} Q\left(S^{\prime}, a\right)-Q(S, A)\right]`

            :math:`S \leftarrow S^{\prime}`

        直到 :math:`S` 是终点

Q-learning的备份图是什么？规则（6.8）更新状态-动作对，因此顶点（更新的根）必须是一个小的，填充的动作节点。
更新也 *来自* 动作节点，最大化在下一个状态下可能执行的所有操作。因此，备份图的底部节点应该是所有这些动作节点。
最后，请记住，我们指出将这些“下一个动作”节点的最大值放在它们之间（图3.4-右）。你能猜出现在的图是什么样吗？
如果能，请在转到图6.4中的答案之前进行猜测。

.. figure:: images/the_cliff_gridworld.png
    :width: 350px
    :align: right

**例6.6：悬崖行走** 这个网格世界示例比较了Sarsa和Q-learning，突出了在策略（Sarsa）和离策略（Q-learning）方法之间的区别。
考虑右边显示的网格世界。这是一个标准的未折扣的，偶然的任务，具有开始和目标状态，以及向上，向下，向右和向左移动的常见操作。
所有过渡的奖励都是 :math:`1`，除了那些标记为“悬崖”的区域。进入该区域会产生 :math:`-100` 的奖励，并且会立即回到起点。

.. figure:: images/performance_of_Sarsa_and_Q-learning.png
    :width: 350px
    :align: right

右图显示了具有 :math:`\varepsilon` -贪婪动作选择的Sarsa和Q-learning方法的性能，:math:`\alpha=0.1`。
在初始瞬态之后，Q-learning会学习最优策略的价值，这些策略沿着悬崖边缘行进。
不幸的是，由于“:math:`\varepsilon` -贪婪动作选择”，这导致它偶尔从悬崖上掉下来。
另一方面，Sarsa将动作选择考虑在内，并学习通过网格上部的更长但更安全的路径。
虽然Q-learning实际上学习了最优策略的价值，其在线表现比学习迂回策略的Sarsa差。
当然，如果 :math:`\varepsilon` 逐渐减少，那么两种方法都会渐近地收敛到最优策略。

*练习6.11* 为什么Q-learning被认为是一种 *离策略* 控制方法？

*练习6.12* 假设动作选择是贪婪的。Q-learning与Sarsa的算法完全相同吗？他们会做出完全相同的动作选择和权重更新吗？


6.6 预期的Sarsa
---------------

考虑与Q-learning一样的学习算法，区别在于其考虑到当前策略下每个动作的可能性，使用预期值而不是最大化下一个状态-动作对。
也就是说，考虑具有如下更新规则的算法

.. math::

    \begin{aligned}
    Q\left(S_{t}, A_{t}\right) & \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \mathbb{E}_{\pi}\left[Q\left(S_{t+1}, A_{t+1}\right) | S_{t+1}\right]-Q\left(S_{t}, A_{t}\right)\right] \\
    & \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma \sum_{a} \pi\left(a | S_{t+1}\right) Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)\right] & \text{(6.9)}
    \end{aligned}

但这遵循Q-learning的模式。给定下一个状态 :math:`S_{t+1}`，
该算法 *确定地* 在与Sarsa *期望* 移动的方向相同的方向上移动，
因此它被称为 *预期的Sarsa*。其备份图如图6.4右边所示。

预期的Sarsa在计算上比Sarsa更复杂，但作为回报，它消除了由于随机选择 :math:`A_{t+1}` 而导致的差异
基于相同的经验，我们可能期望它的表现略好于Sarsa，实际上它通常也是如此。
图6.3显示了预期Sarsa与Sarsa，Q-learning的悬崖行走任务的总结。
预期的Sarsa在这个问题上保留了Sarsa对Q-learning的显着优势。
此外，对于广泛得步长参数 :math:`\alpha`的值，预期Sarsa显示出相对于Sarsa的显着改善。
在悬崖行走中，状态转换都是确定性的，所有随机性都来自策略。
在这种情况下，预期的Sarsa可以安全地设置 :math:`\alpha=1` 而不会导致渐近性能的任何退化，
而Sarsa只能在长期运行时以小的 :math:`\alpha` 值表现良好，短期表现较差。
在这个和其他例子中，预期的Sarsa相对于Sarsa具有一致的经验优势。


.. figure:: images/figure-6.3.png

    图6.3：TD控制方法对悬崖行走任务的临时和渐近性能是关于 :math:`\alpha` 的函数。
    所有算法都使用 :math:`\varepsilon` -贪婪策略，其中 :math:`\varepsilon=1`。
    渐近性能是超过100,000回合的平均，而临时性能是前100回合的平均值。
    这些数据分别是临时和渐近情况的超过50,000回合和10次运行的平均。
    实心圆圈标志着每种方法的最佳临时性能。改编自van Seijen et al.(2009)。

.. figure:: images/figure-6.4.png

    图6.4：Q-learning和预期Sarsa的备份图。

在这些悬崖行走任务结果中，预期的Sarsa被用于策略，但总的来说，它可能使用与目标策略 :math:`\pi` 不同的策略来产生行为，
在这种情况下，它成为一种离策略算法。例如，假设 :math:`\pi` 是贪婪的策略，而行为更具探索性；
然后预期Sarsa正是Q-learning。在这个意义上，预期的Sarsa包含并概括了Q-learning，同时可靠地改善了Sarsa。
除了额外的计算成本之外，预期的Sarsa可能完全支配其他更著名的TD控制算法。


6.8 最大化偏差和双重学习
-------------------------


6.9 游戏，后遗症和其他特殊情况
------------------------------


6.10 总结
-----------


书目和历史评论
---------------


.. [1]
    如果这是一个控制问题，目的是最大限度地缩短旅行时间，那么我们当然会将奖励作为经过时间的 *负* 影响。
    但是因为我们只关注预测（策略评估），所以我们可以通过使用正数来保持简单。
