第9章 在策略预测近似方法
===========================

在本章中，我们开始研究强化学习中的函数近似，考虑其在从在策略数据中估计状态-价值函数的用途，
即从使用已知策略 :math:`\pi` 生成的经验近似 :math:`v_\pi`。
本章的新颖之处在于，近似值函数不是用表格表示，
而表示成是具有权重向量 :math:`\mathbf{w} \in \mathbb{R}^{d}` 的参数化函数形式。
我们将权重向量 :math:`\mathbf{w}` 的状态 :math:`s` 的近似值写作 :math:`\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)`。
例如，:math:`\hat{v}` 可能是状态特征中的线性函数，:math:`\mathbf{w}` 是特征权重的向量。
更一般地，:math:`\hat{v}` 可以是由多层人工神经网络计算的函数，其中 :math:`\mathbf{w}` 是所有层中的连接权重的向量。
通过调整权重，网络可以实现各种不同函数中的任何一种。
或者 :math:`\hat{v}` 可以是由决策树计算的函数，其中 :math:`\mathbf{w}` 是定义树的分裂点和叶值的所有数字。
通常，权重的数量（:math:`\mathbf{w}` 的维数）远小于状态的数量（:math:`d\ll|\mathcal{S}|`），
并且改变一个权重会改变许多状态的估计值。因此，当更新单个状态时，更改会从该状态推广到许多其他状态的值。
这种 *泛化* 使得学习可能更强大，但也可能更难以管理和理解。

也许令人惊讶的是，将强化学习扩展到函数近似也使其适用于部分可观察到的问题，其中个体无法获得完整状态。
如果 :math:`\hat{v}` 的参数化函数形式不允许估计值依赖于状态的某些方面，那么就好像这些方面是不可观察的。
实际上，本书这一部分中使用函数近似的方法的所有理论结果同样适用于部分可观察的情况。
然而，函数近似不能做的是用过去观察的记忆来增强状态表示。第17.3节简要讨论了一些可能的进一步扩展。


9.1 价值函数近似
-------------------

本书中涉及的所有预测方法都被描述为对估计价值函数的更新，该函数将其在特定状态下的值转换为该状态的“备份值”或 *更新目标*。
让我们使用符号 :math:`s \mapsto u` 的表示单独更新，其中 :math:`s` 是更新的状态，
:math:`u` 是 :math:`s` 的估计价值转移到的更新目标。
例如，价值预测的蒙特卡洛更新是 :math:`S_{t} \mapsto G_{t}`，
TD(0)更新是 :math:`S_{t} \mapsto R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbf{w}_{t})`，
n步TD更新为 :math:`S_{t} \mapsto G_{t:t+n}`。在DP（动态规划）中策略评估更新，
:math:`s\mapsto\mathbb{E}_{\pi}\left[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_{t})|S_{t}=s\right]`，
任意状态 :math:`s` 被更新，而在其他情况下，实际经验中遇到的状态 :math:`S_t` 被更新。

将每个更新解释为指定价值函数的所需输入-输出行为的示例是很自然的。
从某种意义上说，更新 :math:`s \mapsto u` 表示状态 :math:`s` 的估计值应更像更新目标 :math:`u`。
到目前为止，实际的更新是微不足道的：:math:`s` 的估计值的表条目已经简单地转移到了 :math:`u` 的一小部分，
并且所有其他状态的估计值保持不变。现在，我们允许任意复杂和精致方法来实现更新，并在 :math:`s` 处进行更新，以便更改许多其他状态的估计值。
学习以这种方式模拟输入输出示例的机器学习方法称为 *监督学习* 方法，当输出是像 :math:`u` 的数字时，该过程通常称为 *函数近似*。
函数近似方法期望接收它们试图近似的函数的期望输入-输出行为的示例。
我们使用这些方法进行价值预测，只需将每次更新的 :math:`s \mapsto u` 作为训练样例传递给它们。
然后，我们将它们产生的近似函数解释为估计价值函数。

以这种方式将每个更新视为传统的训练示例使我们能够使用任何广泛的现有函数近似方法来进行价值预测。
原则上，我们可以使用任何方法进行监督学习，例如人工神经网络，决策树和各种多元回归。
然而，并非所有函数近似方法都同样适用于强化学习。最复杂的人工神经网络和统计方法都假设一个静态训练集，在其上进行多次传递。
然而，在强化学习中，学习能够在线进行，而个体与其环境或其环境模型进行交互是很重要的。
要做到这一点，需要能够从增量获取的数据中有效学习的方法。
此外，强化学习通常需要函数近似方法能够处理非平稳目标函数（随时间变化的目标函数）。
例如，在基于GPI（广义策略迭代）的控制方法中，我们经常寻求在 :math:`\pi` 变化时学习 :math:`q_\pi`。
即使策略保持不变，如果训练示例的目标值是通过自举方法（DP和TD学习）生成的，则它们也是非平稳的。
不能轻易处理这种非平稳性的方法不太适合强化学习。


9.2 预测目标（:math:`\overline{\mathrm{VE}}`）
----------------------------------------------

到目前为止，我们尚未指定明确的预测目标。在表格的情况下，不需要连续测量预测质量，因为学习价值函数可以精确地等于真值函数。
此外，每个状态的学习价值都是分离的──一个状态的更新不受其他影响。
但是通过真正的近似，一个状态的更新会影响许多其他状态，并且不可能使所有状态的值完全正确。
假设我们有比权重更多的状态，所以使一个状态的估计更准确总是意味着让其他的不那么准确。
我们有义务说出我们最关心的状态。
我们必须指定状态分布 :math:`\mu(s)\geq 0,\sum_{s}\mu(s)=1`，表示我们关心每个状态 :math:`s` 中的错误的程度。
通过状态 :math:`s` 中的误差，我们指的是近似值 :math:`\hat{v}(s, \mathbf{w})` 与
真值 :math:`v_\pi(s)` 之间的差的平方。
通过 :math:`\mu` 对状态空间加权，我们得到一个自然目标函数，*均方误差*，表示为 :math:`\overline{\mathrm{VE}}`：

.. math::

    \overline{\mathrm{VE}}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s)\left[v_{\pi}(s)-\hat{v}(s, \mathbf{w})\right]^{2}
    \tag{9.1}

该度量的平方根（根 :math:`\overline{\mathrm{VE}}`）粗略地衡量了近似值与真实值的差异，并且通常用于图中。
通常 :math:`\mu(s)` 被选择为 :math:`s` 中花费的时间的一部分。
在在策略训练中，这被称为 *在策略分布*；我们在本章中完全关注这个案例。
在持续任务中，在策略分布是 math:`\pi` 下的固定分布。

.. admonition:: 回合任务中的在策略分布
    :class: note

    在一个回合任务中，在策略分布略有不同，因为它取决于如何选择回合的初始状态。
    设 ;math:`h(s)` 表示回合在每个状态 :math:`s` 中开始的概率，
    :math:`\eta(s)` 表示在一个回合中状态 :math:`s` 中平均花费的时间步数。
    如果回合以 :math:`s` 开头，或者如果从之前的状态 :math:`\overline{s}` 转换为 :math:`s`，则花费时间在状态 :math:`s` 中：

    .. math::

        \eta(s)=h(s)+\sum_{\overline{s}} \eta(\overline{s}) \sum_{a} \pi(a | \overline{s}) p(s | \overline{s}, a), \text { 对所有 } s \in \mathcal{S}
        \tag{9.2}

    可以针对预期的访问次数 :math:`\eta(s)` 求解该方程组。 然后，在策略分布是每个状态所花费的时间的一小部分，标准化和为一：

    .. math::

        \mu(s)=\frac{\eta(s)}{\sum_{s^{\prime}} \eta(s^{\prime})}, \quad \text { 对所有 } s \in \mathcal{S}
        \tag{9.3}

    这是没有折扣的自然选择。如果存在折扣（:math:`\gamma<1`），则应将其视为终止形式，
    这可以简单通过在（9.2）的第二项中包含因子 :math:`\gamma` 来完成。

这两种情况，即持续的和回合的，表现相似，但近似时必须在形式分析中单独处理，
正如我们将在本书的这一部分中反复看到的那样。这完成了学习目标的规范。

目前还不完全清楚 :math:`\overline{\mathrm{VE}}` 是加强学习的正确性能目标。
请记住，我们的最终目的──我们学习价值函数的原因──是找到更好的策略。
用于此目的的最佳价值函数不一定是最小化 :math:`\overline{\mathrm{VE}}` 的最佳值。
然而，目前尚不清楚价值预测的更有用的替代目标是什么。目前，我们将专注于 :math:`\overline{\mathrm{VE}}`。

就 :math:`\overline{\mathrm{VE}}` 而言，理想的目标是找到一个 *全局最优值*，
一个权重向量 :math:`\mathbf{w}^{*}`，对于所有可能的 :math:`\mathbf{w}`，
:math:`\overline{\mathrm{VE}}(\mathbf{w}^{*})\leq\overline{\mathrm{VE}}(\mathbf{w})`。
对于诸如线性函数近似器之类的简单函数逼近器，有时可以实现这一目标，
但对于诸如人工神经网络和决策树之类的复杂函数近似器来说很少是可能的。
除此之外，复杂函数近似器可以寻求收敛到 *局部最优*，一个权重向量 :math:`\mathbf{w}`，
对于 :math:`\mathbf{w}^{*}` 的某些邻域中的所有 :math:`\mathbf{w}` 满足
:math:`\overline{\mathrm{VE}}(\mathbf{w}^{*})\leq\overline{\mathrm{VE}}(\mathbf{w})`。
虽然这种保证只是稍微让人放心，但对于非线性函数近似器来说，它通常是最好的，而且通常它就足够了。
尽管如此，对于许多对强化学习感兴趣的情况，并不能保证收敛到最佳，甚至在最佳的有界距离内。
事实上，有些方法可能会出现发散，其 :math:`\overline{\mathrm{VE}}` 极限趋于无穷。

在前两节中，我们概述了一个框架，用于将价值预测的各种强化学习方法与各种函数近似方法相结合，使用前者的更新为后者生成训练样本。
我们还描述了这些方法可能希望最小化的 :math:`\overline{\mathrm{VE}}` 性能测量。
可能的函数近似方法的范围太大以至于不能覆盖所有方法，并且无论如何对其中的大多数方法进行可靠的评估或推荐知之甚少。
必要时，我们只考虑几种可能性。在本章的剩余部分，我们将重点放在基于梯度原理的函数近似方法，特别是线性梯度下降方法上。
我们关注这些方法的部分原因是因为我们认为这些方法特别有前途，因为它们揭示了关键的理论问题，同时也因为它们很简单，而且我们的空间有限。


9.3 随机梯度和半梯度方法
--------------------------

我们现在详细地开发一类用于价值预测中的函数近似的学习方法，这些方法基于随机梯度下降（SGD）。
SGD方法是所有函数近似方法中使用最广泛的方法之一，尤其适用于在线强化学习。

在梯度下降方法中，权重向量是具有固定数量的实值分量的
列向量 :math:`\mathbf{w} \doteq(w_{1}, w_{2}, \ldots, w_{d})^{\top}` [1]_，
近似值函数 :math:`\hat{v}(s, \mathbf{w})` 是
所有 :math:`s\in\mathcal{S}` 的 :math:`\mathbf{w}` 的可微函数。
我们将在一系列离散时间 :math:`t = 0,1,2,3,\dots,` 的每一个中更新 :math:`\mathbf{w}`，
因此我们需要符号 :math:`\mathbf{w}_t` 表示每一步的权重向量。
现在，让我们假设，在每一步，我们都观察到一个新的样例 :math:`S_{t} \mapsto v_{\pi}\left(S_{t}\right)`
由一个（可能是随机选择的）状态 :math:`S_t` 和它在策略下的真实值组成。
这些状态可能是与环境交互的连续状态，但是现在我们不这么认为。
即使我们给出了每个 :math:`S_t` 的精确而正确的价值 :math:`v_{\pi}\left(S_{t}\right)`，
这仍然存在一个难题，因为我们的函数近似器具有有限的资源并因此具有有限的进精度（resolution）。
特别是，通常没有 :math:`\mathbf{w}` 能够使所有状态，甚至所有样例都完全正确。
此外，我们必须推广到未出现在示例中的所有其他状态。

我们假设状态出现在具有相同分布 :math:`\mu` 的样例中，
我们试图通过它来最小化由（9.1）给出的 :math:`\overline{\mathrm{VE}}`。
在这种情况下，一个好的策略是尽量减少观察到的样例的错误。
*随机梯度下降* （SGD）方法通过在每个样例之后将权重向量向最大程度地减少该示例中的误差的方向少量调整来实现此目的：

.. math::

    \begin{aligned}
    \mathbf{w}_{t+1} & \doteq \mathbf{w}_{t}-\frac{1}{2} \alpha \nabla\left[v_{\pi}\left(S_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right]^{2} & \text{(9.4)} \\
    &=\mathbf{w}_{t}+\alpha\left[v_{\pi}\left(S_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) & \text{(9.5)}
    \end{aligned}

其中 :math:`\alpha` 是正步长参数，对于作为向量函数（这里是 :math:`\mathbf{w}`）的
任何标量表达式 :math:`f(\mathbf{w})`，:math:`\nabla f(\mathbf{w})` 定义为关于向量分量
表达式的偏导数的列向量，向量分量为：

.. math::

    \nabla f(\mathbf{w}) \doteq\left(\frac{\partial f(\mathbf{w})}{\partial w_{1}}, \frac{\partial f(\mathbf{w})}{\partial w_{2}}, \dots, \frac{\partial f(\mathbf{w})}{\partial w_{d}}\right)^{\top}
    \tag{9.6}

该导数向量是 math:`f` 相对于 :math:`\mathbf{w}` 的梯度。SGD方法是“梯度下降”方法，
因为 :math:`\mathbf{w}_t` 中的整个步长与示例的平方误差（9.4）的负梯度成比例。这是误差下降最快的方向。
当像这儿仅在单个可能是随机选择的样例上完成更新时，梯度下降方法被称为“随机”。
在许多样例中，采取小步，总体效果是最小化如VE的平均性能测量。

可能不会立即明白为什么SGD在梯度方向上只迈出一小步。难道我们不能一直朝这个方向移动并完全消除样例中的误差吗？
在许多情况下，这可以做到，但通常是不可取的。请记住，我们不会寻找或期望找到一个对所有状态都没有误差的价值函数，
而只是一个平衡不同状态误差的近似值。如果我们在一个步骤中完全纠正每个样例，那么我们就找不到这样的平衡。
事实上，SGD方法的收敛结果假设 :math:`\alpha` 随着时间的推移而减少。
如果它以满足标准随机近似条件（2.7）的方式减小，则SGD方法（9.5）保证收敛到局部最优。

我们现在转向第 :math:`t` 个训练样例 :math:`S_{t} \mapsto U_{t}` 的目标输出
（此处表示为 :math:`U_{t}\in\mathbb{R}`）不是真值 :math:`v_\pi(S_t)` 而是一些（可能是随机的）近似的情况。
例如，:math:`U_{t}` 可能是 :math:`v_\pi(S_t)` 的噪声损坏版本，
或者它可能是使用上一节中提到的 :math:`\hat{v}` 的自举目标之一。
在这些情况下，我们无法执行精确更新（9.5），因为 :math:`v_\pi(S_t)` 是未知的，
但我们可以通过用 :math:`U_t` 代替 :math:`v_\pi(S_t)` 来近似它。
这产生了以下用于状态价值预测的一般SGD方法：

.. math::

    \mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha\left[U_{t}-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)
    \tag{9.7}

如果 :math:`U_t` 是无偏估计，即，如果对于每个 :math:`t` 有
:math:`\mathbb{E}\left[U_{t} | S_{t}=s\right]=v_{\pi}(S_{t})`，
则 :math:`\mathbf{w}_t` 保证对于减小的 :math:`\alpha` 在通常的随机近似条件（2.7）下收敛到局部最优值。

例如，假设示例中的状态是通过使用策略 :math:`\pi` 与环境交互（或模拟交互）生成的状态。
因为状态的真实值是跟随它的回报的预期值，
所以蒙特卡洛目标 :math:`U_{t}\doteq G_{t}` 根据定义是 :math:`v_\pi(S_t)` 的无偏估计。
通过这种选择，一般SGD方法（9.7）收敛于 :math:`v_\pi(S_t)` 的局部最佳近似。
因此，蒙特卡洛状态价值预测的梯度下降版本保证找到局部最优解。完整算法的伪代码如下框所示。

.. admonition:: 梯度蒙特卡罗算法估计 :math:`\hat{v} \approx v_{\pi}`
    :class: important

    输入：要评估的策略 :math:`\pi`。

    输入：可微分函数 :math:`\hat{v} : \mathcal{S} \times \mathbb{R}^{d} \rightarrow \mathbb{R}`

    算法参数：步长 :math:`\alpha>0`

    任意初始化价值函数权重 :math:`\mathbf{w} \in \mathbb{R}^{d}` （例如，:math:`\mathbf{w}=\mathbf{0}`）

    一直循环（对每一个回合）：

        使用 :math:`\pi` 生成一个回合 :math:`S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, \ldots, R_{T}, S_{T}`

        对回合的每一步循环，:math:`t=0,1, \ldots, T-1`：

            :math:`\mathbf{w} \leftarrow \mathbf{w}+\alpha\left[G_{t}-\hat{v}(S_{t}, \mathbf{w})\right] \nabla \hat{v}(S_{t}, \mathbf{w})`

如果将 :math:`v_\pi(S_t)` 的自举估计用作（9.7）中的目标 :math:`U_t`，则不能获得相同的保证。
自举目标如n步回报 :math:`G_{t : t+n}` 或DP目标
:math:`\sum_{a,s^{\prime},r}\pi(a | S_{t}) p(s^{\prime}, r | S_{t}, a)\left[r+\gamma \hat{v}(s^{\prime}, \mathbf{w}_{t})\right]`
全部依赖关于权重向量 :math:`\mathbf{w}_t` 的当前值，这意味着它们将被偏置并且它们将不会产生真正的梯度下降方法。
一种看待这一点的方法是从（9.4）到（9.5）的关键步骤依赖于目标独立于 :math:`\mathbf{w}_t`。
如果使用自举估计代替 :math:`v_\pi(S_t)`，则该步骤无效。自举方法实际上不是真正的梯度下降的实例（Barnard，1993）。
它们考虑了改变权重向量 :math:`\mathbf{w}_t` 对估计的影响，但忽略了它对目标的影响。
它们只包括渐变的一部分，因此，我们将它们称为 *半梯度方法*。

尽管半梯度（自举）方法不像梯度方法那样稳健地收敛，但它们在重要情况下可靠地收敛，例如下一节中讨论的线性情况。
而且，它们提供了重要的优点，使它们通常是明显优选的。
这样做的一个原因是它们通常能够显着加快学习速度，正如我们在第6章和第7章中看到的那样。
另一个原因是它们使学习能够连续和在线，而无需等待回合的结束。这使它们能够用于持续的问题并提供计算优势。
一种原型半梯度方法是半梯度TD(0)，
其使用 :math:`U_{t} \doteq R_{t+1}+\gamma \hat{v}(S_{t+1}, \mathbf{w})` 作为其目标。
下面的框中给出了该方法的完整伪代码。

.. admonition:: 半梯度TD(0)估计 :math:`\hat{v} \approx v_{\pi}`
    :class: important

    输入：要评估的策略 :math:`\pi`。

    输入：可微分函数 :math:`\hat{v} : \mathcal{S}^{+} \times \mathbb{R}^{d} \rightarrow \mathbb{R}` 使得 :math:`\hat{v}(\text{终止}, \cdot)=0`

    算法参数：步长 :math:`\alpha>0`

    任意初始化价值函数权重 :math:`\mathbf{w} \in \mathbb{R}^{d}` （例如，:math:`\mathbf{w}=\mathbf{0}`）

    一直循环（对每一个回合）：

        初始化 :math:`S`

        对回合的每一步循环：

            选择 :math:`A \sim \pi(\cdot | S)`

            采取动作 :math:`A`，观察 :math:`R`，:math:`S^{\prime}`

            :math:`\mathbf{w} \leftarrow \mathbf{w}+\alpha\left[R+\gamma \hat{v}\left(S^{\prime}, \mathbf{w}\right)-\hat{v}(S, \mathbf{w})\right] \nabla \hat{v}(S, \mathbf{w})`

            :math:`S \leftarrow S^{\prime}`

        直到 :math:`S` 终止

状态 *聚合* 是泛化函数近似的简单形式，其中状态被分组在一起，每个组具有一个估计值（权重向量 :math:`\mathbf{w}` 的一个分量）。
状态的值被估计为其组的分量，并且当状态被更新时，仅更新该分量。状态聚合是SGD（9.7）的特例，
其中梯度 :math:`\nabla \hat{v}(S_{t}, \mathbf{w}_{t})`
对于 :math:`S_t` 的分量为1，对于其他成分为0。

**例9.1：1000状态随机行走的状态聚合** 考虑1000状态版本的随机行走任务（示例6.2和7.1）。
状态从1到1000，从左到右编号，并且所有回合在中心附近状态500开始。
状态转换从当前状态到其左边的100个邻近状态之一，或者其左边的100个邻近状态之一，都具有相同的概率。
当然，如果当前状态接近边缘，那么在它的那一侧可能少于100个邻居。
在这种情况下，进入那些丢失的邻居的所有概率都会进入在那一侧终止的概率
（因此，状态1有0.5的机会在左边终止，而状态950有0.25的机会在右侧终止）。
像往常一样，左边的终止产生 :math:`-1` 的奖励，右边的终止产生 :math:`+1` 的奖励。
所有其他过渡奖励都为零。我们在本节中将此任务用作运行示例。

.. figure:: images/figure-9.1.png

    **图9.1：** 使用梯度蒙特卡罗算法对1000状态随机行走任务进行状态聚合的函数近似。

图9.1显示了此任务的真值函数 :math:`v_\pi`。它几乎是一条直线，每端最后100个状态向水平方向略微弯曲。
图还显示了梯度蒙特卡罗算法通过状态聚合学习的最终近似价值函数，
此状态聚合具有步长为 :math:`\alpha=2×10^{}-5` 的100,000个回合。
对于状态聚合，1000个状态被分成10组，每组100个状态（即，状态1-100是一组，状态101-200是另一组，等等）。
图中所示的阶梯效应是典型的状态聚合；在每个组中，近似值是恒定的，并且它从一个组突然改变到下一个组。
这些近似值接近 :math:`\overline{\mathrm{VE}}` 的全局最小值（9.1）。

通过参考该任务的状态分布 :math:`\mu`，可以最好地理解近似价值的一些细节，如图的下部所示，标度在右侧。
位于中心的状态500是每个回合的第一个状态，但很少再次访问。平均而言，约有1.37％的时间步花费在开始状态。
从开始状态开始一步到达的状态是访问次数最多的状态，其中每个步骤花费大约0.17％的时间步。
从那里 :math:`\mu` 几乎线性地下降，在极端状态1和1000处达到约0.0147％。
分布的最明显效果是在最左边的组上，其值明显偏移到高于组内状态的真实值的未加权平均值，以及在最右边的小组，其价值明显向低的方向移动。
这是由于这些区域中的状态在权重上具有最大的不对称性 :math:`\mu`。
例如，在最左边的组中，状态100的加权比状态1强3倍以上。因此，对组的估计偏向于状态100的真实值，其高于状态1的真实值。


9.4 线性方法
----------------


9.5 线性方法的特征构造
------------------------

9.5.1 多项式
^^^^^^^^^^^^^^^

9.5.2 傅立叶基
^^^^^^^^^^^^^^^^^

9.5.3 粗编码（Coarse Coding）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

9.5.4 平铺编码（Tile Coding）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

9.5.5 径向基函数
^^^^^^^^^^^^^^^^^^^


9.6 手动选择步长参数
-----------------------


9.7 非线性函数近似：人工神经网络
---------------------------------


9.8 最小二乘TD
----------------------


9.9 基于内存的函数近似
-------------------------


9.10 基于核的函数近似
------------------------


9.11 深入研究在策略学习：兴趣和重点
------------------------------------


9.12 总结
--------------


书目和历史评论
---------------


.. [1]
    :math:`^{\top}` 表示转置，此处需要将文本中的水平行向量转换为垂直列向量；
    在本书中，除非明确地水平写入或转置，否则向量通常被认为是列向量。
