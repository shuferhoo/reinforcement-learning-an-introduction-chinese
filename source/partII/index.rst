=====================
第二部分 近似解决方法
=====================

在本书的第二部分，我们扩展了第一部分中介绍的表格方法，以适用于任意大的状态空间的问题。
在我们想要应用强化学习的许多任务中，状态空间是组合的和巨大的；例如，可能的相机图像的数量远大于宇宙中的原子数。
在这种情况下，即使在无限时间和数据的限制下，我们也不能期望找到最优策略或最优值函数；
我们的目标是使用有限的计算资源找到一个好的近似解决方案。在本书的这一部分，我们探讨了这种近似的解决方法。

大状态空间的问题不仅仅是大表格所需的内存，而是准确填充它们所需的时间和数据。
在我们的许多目标任务中，几乎所遇到的每个状态都将永远不会被看到。
为了在这种状态下作出明智的决定，有必要从以前的经验中推广出一些与现有状态相似的不同状态。
换句话说，关键问题是 *泛化*。如何有效地推广有限状态空间子集的经验，以便在更大的子集上产生良好的近似？

幸运的是，已经广泛研究了来自实例的泛化，并且我们不需要发明用于强化学习的全新方法。
在某种程度上，我们只需要将强化学习方法与现有的泛化方法相结合。
我们需要的泛化类型通常称为 *函数近似*，因为它从所需函数（例如，价值函数）中获取示例并尝试从它们推广以构造整个函数的近似。
函数逼近是监督学习的一个实例，是机器学习，人工神经网络，模式识别和统计曲线拟合中研究的主要课题。
理论上，在这些领域中研究的任何方法都可以成为强化学习算法中的函数近似器的作用，尽管在实践中，某些方法比其他方法更容易适应这个角色。

具有函数近似的强化学习涉及许多在常规监督学习中通常不会出现的新问题，例如非平稳性，自举和延迟目标。
我们在本部分的五章中先后介绍了这些问题和其他问题。最初，我们将注意力限制在在策略训练上，
处理在第9章中的预测案例，其中给出了策略并且仅对其价值函数进行了近似，
然后在第10章中讨论了控制案例，其中找到了最优策略的近似值。在第11章中讨论了具有函数近似的离策略学习的挑战性问题。
在这三章的每一章中，我们将不得不回到第一原则并重新审视学习的目标，以考虑函数近似。
第12章介绍和分析了 *资格迹* 的算法机制，在很多情况下，它大大提高了多步强化学习方法的计算性能。
本部分的最后一章探讨了一种不同的控制方法 - *策略梯度方法*，它直接近似最优策略，并且永远不需要形成近似价值函数
（如果它们确实接近价值函数和策略，它们可能会更加有效）。


.. toctree::
   :maxdepth: 2

   chapter9/on-policy_prediction_with_approximation
   chapter10/on-policy_control_with_approximation
   chapter11/off-policy_methods_with_approximation
   chapter12/eligibility_traces
   chapter13/policy_gradient_methods
